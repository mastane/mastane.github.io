---
title: "Beyond Log-Concavity: Theory and Algorithm for Sum-Log-Concave Optimization"
collection: publications
permalink: /publication/2023-09-26-xgd
#excerpt: 'This paper is about the number 1. The number 2 is left for future work.'
date: 2023-09-26  # year-month-day
venue: 'preprint'
paperurl: 'https://arxiv.org/pdf/2309.15298.pdf'
#citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---
Abstract. This paper extends the classic theory of convex optimization to the minimization of functions that are equal to the negated logarithm of what we term as a "sum-log-concave" function, i.e., a sum of log-concave functions. In particular, we show that such functions are in general not convex but still satisfy generalized convexity inequalities. These inequalities unveil the key importance of a certain vector that we call the "cross-gradient" and that is, in general, distinct from the usual gradient. Thus, we propose the Cross Gradient Descent (XGD) algorithm moving in the opposite direction of the cross-gradient and derive a convergence analysis. As an application of our sum-log-concave framework, we introduce the so-called "checkered regression" method relying on a sum-log-concave function. This classifier extends (multiclass) logistic regression to non-linearly separable problems since it is capable of tessellating the feature space by using any given number of hyperplanes, creating a checkerboard-like pattern of decision regions.

Download [paper](https://arxiv.org/pdf/2309.15298.pdf)
