---
title: 'Beyond Convexity #1: The Cross-Convex Bestiary'
date: 2023-10-12  # year-month-day
permalink: /posts/2023/10/blog-post-1/
tags:
  - generalized convexity inequality
  - mixture of log-concave distributions
---

In this blog post, we introduce a generalized notion of convexity, that we call "cross-convexity" since it involves additional interaction terms compared to standard convexity. We will only focus on a particular case but we refer the reader to Lemma 1 in ["Beyond Log-Concavity: Theory and Algorithm for Sum-Log-Concave Optimization"](https://arxiv.org/abs/2309.15298) for a more general statement.
Let us first recall that a differentiable function $f:\mathbb{R}^d \rightarrow$ is convex if it dominates all its tangent hyperplanes:
$$\forall a, \forall x, f(x) \ge f(a) + \nabla f(a)^\top (x-a).$$

$\bullet$ A very important characteristic of the family of convex functions is that they are closed under (i) summation and (ii) affine reparametrization.
Formally, (i) if $f_1,f_2$ are two convex functions, then $f_1+f_2$ is also convex ; and (ii) if $f$ is convex, then $x \mapsto f(Ax+b)$ is also convex.
These two closure properties are very important in machine learning where the objective function is typically equal to a sum over the dataset with affine neuronal transformations of the input data.
A first challenge when trying to generalize convexity for ML applications is to pick a family of functions satisfying these such closedness under summation and affine reparametrization.
For instance, while the notion of [quasi-convexity](https://en.wikipedia.org/wiki/Quasiconvex_function) is often cited as a natural extension of standard convexity, unfortunately it is not closed under summation since the sum of two quasi-convex functions is not necessarily quasi-convex.

$\bullet$ Note that the right-hand side in the convexity inequality, namely "$\ge f(a) + \nabla f(a)^\top (x-a)$", represents the tangent hyperplane of $f$ at the point $a$.
In particular, this lower bound is a critical ingredient in the analysis of gradient descent (see e.g. [Bach's LTFP book](https://www.di.ens.fr/~fbach/ltfp_book.pdf)).
### Collection of cross-convex functions (in red) with their tangent surface (in green): [https://github.com/mastane/TheXCB](https://github.com/mastane/TheXCB)

### The green tangent surface represents the lower bound in a generalized convexity inequality (see )

### Steps
* Given log-concave function $p$
* Compute cross-convex function $F(x,y)=-\log(p(x)+p(y))$
* Tangent lower bound $\mathcal{T}_{a,b}(x, y) \le F(x,y)$ at point $(a,b)$:
$$\mathcal{T}_{a,b}(x, y) = F(a,b) + \nabla F(a,b)^\top \begin{pmatrix} x-a \\ y-b \end{pmatrix} - D_{\text{KL}}\left( \begin{pmatrix} \frac{p(a)}{p(a)+p(b)} \\ \frac{p(b)}{p(a)+p(b)} \end{pmatrix} \, \Bigg \| \, \begin{pmatrix} \frac{p(x)}{p(x)+p(y)} \\ \frac{p(y)}{p(x)+p(y)} \end{pmatrix} \right)$$
* <u>Note:</u> _Actually, the negative sign in front of the KL is bad news for the analysis of gradient descent...check out my paper to see how to solve that issue, by considering a reweighted version of the gradient_

### $$\mathfrak{B}$$estiary

* $$\mathcal{G}$$<u>aussian mixture</u>
<img src="https://mastane.github.io/images/XCB/gifs/rotating_plot_00_Gaussian.gif" width="100%" height="100%">

* $$\mathcal{L}$$<u>ogistic mixture</u>
<img src="https://mastane.github.io/images/XCB/gifs/rotating_plot_00_Logistic.gif" width="100%" height="100%">

* $$\mathcal{H}$$<u>yperbolic</u> $$\mathcal{S}$$<u>ecant mixture</u>
<img src="https://mastane.github.io/images/XCB/gifs/rotating_plot_00_Sech.gif" width="100%" height="100%">

* $$\mathcal{G}$$<u>umbel mixture</u>
<img src="https://mastane.github.io/images/XCB/gifs/rotating_plot_00_Gumbel.gif" width="100%" height="100%">


------
