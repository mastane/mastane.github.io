---
title: 'Beyond Convexity #2: Barygradient flow'
date: 2024-12-07  # year-month-day
permalink: /posts/2024/12/blog-post-2/
tags:
  - gradient flow
  - baryconvex optimization
  - Bregman divergence
---

In [my latest paper](https://arxiv.org/pdf/2411.00928), I introduced a generalized proximal point algorithm (PPA): given a point $(x,q) \in \mathbb{R}^m \times \Delta_S$ ($m,S \ge 1$ and $\Delta_S$ the probability simplex), the next iterate $(x',q')$ is given by:
$$
( \nabla f + \lambda A )(x',q') = \nabla f(x,q)
$$
for step-size $\lambda>0$, $f(x,q)=\frac12 ||x||^2 + h(q)$, with $h(q)=\sum_{s=1}^S q_s \log(q_s)$ the negentropy and
$$
A(x,q) = \begin{pmatrix}
J_\ell(x)^\intercal q \\
-\ell(x)
\end{pmatrix}
$$
where $J_\ell$ denotes the Jocobian matrix of $\ell=(\ell_1,\dots,\ell_S):\mathbb{R}^m \rightarrow \mathbb{R}^S$ with each $\ell_s$ convex ($\forall 1\le s \le S$).

In particular, I showed that $A$ is a monotone operator and that the $f$-resolvent $(\nabla f + \lambda A)^{-1} \circ \nabla f$ is Bregman firmly nonexpansive with respect to the Bregman divergence $D_f$.
In other words, this ensure that this generalized PPA will converge to a fixed point, if there exists any.

In this blog post, we propose to generalize the gradient flow ODE by letting $\lambda \rightarrow 0$ in our generalized PPA (for a great introduction to gradient flow, see [Bach's blog post](https://francisbach.com/gradient-flows/)).

<u>Definition:</u> Denote $F(x,q) = q^\intercal \ell(x)$. Then, we define the barygradient flow ODE as
$$
\dot \zeta(t) = - \begin{pmatrix}
I_m & 0 \\
0 & -I_S
\end{pmatrix} \nabla F( (\nabla f)^{-1}( \zeta(t) ) ) ,
$$
where $\zeta : \mathbb{R}_+ \rightarrow \mathbb{R}^m \times \nabla h(\Delta_S) $.

------
